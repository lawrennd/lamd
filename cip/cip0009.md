---
author: "lawrennd"
created: "2026-01-04"
id: "0009"
last_updated: "2026-01-04"
status: "Implemented"
related_requirements: ["0005"]
tags:
- cip
- performance
- optimization
- benchmarking
title: "Further Performance Optimization: Close the Gap"
---

# CIP-0009: Further Performance Optimization: Close the Gap

## Summary

While CIP-0008 achieved a 4x overall speedup (151s → 37s), there's a significant gap between the **per-call speedup (13.9x)** and the **overall pipeline speedup (4x)**. This CIP proposes systematic benchmarking and optimization to close that gap and achieve even faster builds.

**Result**: All 3 phases completed successfully, achieving **13.2x overall speedup (45s → 3.4s)**, far exceeding the 6-8x target!

## Status

**Current**: Implemented ✅

## Motivation

### Current Performance (CIP-0008 Baseline)

| Metric | Before | After | Speedup |
|--------|--------|-------|---------|
| **Per mdfield call** | 1.25s | 0.09s | **13.9x** |
| **Overall talk build** | 151s | 37s | **4.0x** |

**The Gap**: 13.9x vs 4x suggests ~75% of build time is spent on non-mdfield operations.

### Why Optimize Further?

1. **User Experience**: Faster iteration cycles for talk/CV development
2. **Efficiency**: Reduce resource consumption for large-scale builds
3. **Understanding**: Identify other bottlenecks in the pipeline
4. **Best Practices**: Establish performance baseline for future work

### What's Taking Time?

For a 37-second build with 22 mdfield calls:
- **mdfield calls**: 22 × 0.09s ≈ **2s** (5% of total time)
- **Other operations**: 37s - 2s ≈ **35s** (95% of total time)

**Unknown**: What are those other 35 seconds doing?

## Objectives

### Primary Goals

1. **Profile the entire build pipeline** to identify bottlenecks
2. **Categorize time spent** in each build phase
3. **Optimize high-impact operations** (Pareto principle: 80/20)
4. **Achieve 6-8x overall speedup** (target: 151s → 20-25s range)

### Secondary Goals

5. **Establish performance testing framework** for regression detection
6. **Document best practices** for fast builds
7. **Create optimization guide** for users

## Proposed Approach

### Phase 1: Comprehensive Profiling

**Instrument the build pipeline** to measure:

1. **maketalk/makecv setup time**:
   - Argument parsing
   - Config file loading (`_lamd.yml`)
   - Interface validation
   - Git operations (`git pull` on dependencies)

2. **Makefile generation time**:
   - File I/O
   - Template processing

3. **Make execution time** (breakdown):
   - Each mdfield call (already optimized)
   - Preprocessor (`mdpp`) execution
   - Dependency scanning (`dependencies` command)
   - File I/O and cache checks
   - Pandoc execution (slides, notes, etc.)
   - Bibliography processing
   - Diagram copying/processing

4. **External tool overhead**:
   - Git operations
   - File system operations
   - Process spawning

**Tool**: Add `--profile` flag to `maketalk`/`makecv`:
```bash
maketalk my-talk.md --profile
# Outputs detailed timing breakdown
```

### Phase 2: Low-Hanging Fruit

**Quick wins** (likely candidates based on typical build patterns):

1. **Parallel mdfield calls**:
   - Currently: 22 sequential calls in Makefile
   - Proposal: Batch or parallelize using GNU make's `-j` flag
   - Expected: 2s → 0.2s (if 10x parallelism)

2. **Reduce git pulls**:
   - Currently: Pulls on every build
   - Proposal: Check if already up-to-date first
   - Expected: Save 2-5s per build

3. **Cache preprocessor output**:
   - Currently: Preprocessor runs on every build
   - Proposal: Cache based on file timestamps/hashes
   - Expected: Save 5-10s on unchanged files

4. **Optimize dependency scanning**:
   - Currently: Scans all includes/snippets
   - Proposal: Cache dependency graph
   - Expected: Save 2-5s

5. **Lazy config loading**:
   - Currently: Loads full Interface on every mdfield call
   - Already optimized via server mode
   - Check if any remaining instances

### Phase 3: Deep Optimizations

**Bigger changes** (if needed to hit target):

1. **Incremental builds**:
   - Only regenerate changed outputs
   - Smart dependency tracking
   - Expected: 10-20x for unchanged files

2. **Server-mode for other commands**:
   - `dependencies` command
   - `mdpp` preprocessor
   - `flags` command
   - Expected: 2-5x for these operations

3. **Compiled preprocessor**:
   - Port `mdpp` to compiled language (Rust?)
   - Expected: 5-10x for preprocessing

4. **Parallel pandoc execution**:
   - Generate multiple formats simultaneously
   - Expected: 2x for multi-format builds

### Phase 4: Performance Testing Framework

**Automated regression detection**:

```bash
# Performance test suite
pytest tests/performance/ -v

# Benchmark against baseline
pytest tests/performance/ --benchmark-only
```

**What to test**:
- Single talk build time
- Multi-talk batch build time
- CV build time
- Cold start vs warm cache
- Different talk sizes (small, medium, large)

## Success Criteria

### Minimum Success (Phase 1-2)

- [ ] Complete profiling of build pipeline
- [ ] Categorize time spent in each phase
- [ ] Implement at least 3 quick wins
- [ ] Achieve **5x overall speedup** (151s → 30s)

### Target Success (Phase 1-3)

- [ ] Achieve **6-8x overall speedup** (151s → 20-25s)
- [ ] Document all bottlenecks and optimizations
- [ ] Performance testing framework in place
- [ ] No regression in functionality or reliability

### Stretch Goals (Phase 4)

- [ ] Achieve **10x overall speedup** (151s → 15s)
- [ ] Incremental builds working
- [ ] Performance monitoring in CI/CD

## Implementation Plan

### Step 1: Add Profiling Infrastructure

Create `lamd/profiler.py`:
```python
import time
from contextlib import contextmanager

class BuildProfiler:
    def __init__(self):
        self.timings = {}
    
    @contextmanager
    def measure(self, operation: str):
        start = time.perf_counter()
        yield
        elapsed = time.perf_counter() - start
        self.timings[operation] = elapsed
    
    def report(self):
        total = sum(self.timings.values())
        for op, elapsed in sorted(self.timings.items(), key=lambda x: -x[1]):
            pct = (elapsed / total) * 100
            print(f"{op:40s}: {elapsed:6.2f}s ({pct:5.1f}%)")
```

Usage in `maketalk.py`:
```python
profiler = BuildProfiler()

with profiler.measure("Config Loading"):
    iface = Interface.from_file(...)

with profiler.measure("Git Pull Dependencies"):
    os.system("git pull")

# ... etc

if args.profile:
    profiler.report()
```

### Step 2: Benchmark Current State

Run comprehensive benchmarks:
```bash
# Small talk (few includes)
time maketalk small-talk.md --profile

# Medium talk (typical)
time maketalk medium-talk.md --profile

# Large talk (many includes, diagrams)
time maketalk large-talk.md --profile

# CV build
time makecv my-cv.md --profile
```

Document baseline in `docs/performance-baseline.md`.

### Step 3: Implement Quick Wins

Prioritize by ROI (impact / effort):
1. Conditional git pulls (high impact, low effort)
2. Parallel mdfield calls (medium impact, low effort)
3. Cache dependency scans (medium impact, medium effort)

### Step 4: Measure and Iterate

After each optimization:
- Re-run benchmarks
- Document speedup
- Check for regressions
- Commit with performance data

### Step 5: Performance Testing

Create `tests/performance/test_build_performance.py`:
```python
import pytest
from time import perf_counter

@pytest.mark.benchmark
def test_talk_build_performance(tmp_path):
    """Benchmark typical talk build time."""
    # Setup test talk file
    talk_file = create_test_talk(tmp_path, size="medium")
    
    # Measure build time
    start = perf_counter()
    result = subprocess.run(["maketalk", str(talk_file)])
    elapsed = perf_counter() - start
    
    # Assert performance threshold
    assert result.returncode == 0
    assert elapsed < 30.0, f"Build took {elapsed}s (target: <30s)"
```

## Potential Bottlenecks to Investigate

### 1. Git Operations

**Hypothesis**: `git pull` on every build adds 2-5s
**Test**: Compare build time with/without git operations
**Fix**: Only pull if not up-to-date

### 2. File I/O

**Hypothesis**: Reading/writing many small files is slow
**Test**: Profile file operations
**Fix**: Buffer writes, minimize reads

### 3. Process Spawning

**Hypothesis**: Spawning many subprocesses has overhead
**Test**: Count subprocess calls, measure overhead
**Fix**: Batch operations, use server mode for more tools

### 4. Preprocessor

**Hypothesis**: `mdpp` is slow for large files with many includes
**Test**: Profile preprocessor separately
**Fix**: Cache results, optimize hot paths, or rewrite in compiled language

### 5. Dependency Scanning

**Hypothesis**: Scanning all includes/snippets is redundant
**Test**: Profile `dependencies` command
**Fix**: Cache dependency graph, only rescan on file changes

### 6. Pandoc Execution

**Hypothesis**: Pandoc is slow but necessary
**Test**: Measure pandoc time separately
**Fix**: Parallel execution for multiple formats, check if newer version is faster

## Risks and Mitigations

### Risk 1: Premature Optimization

**Risk**: Optimizing wrong thing, no real benefit
**Mitigation**: Profile first, optimize second. Data-driven decisions.

### Risk 2: Complexity Increase

**Risk**: Caching and incremental builds add complexity
**Mitigation**: Start with simple optimizations. Add complexity only if needed for target.

### Risk 3: Regressions

**Risk**: Optimizations break functionality
**Mitigation**: Comprehensive testing, performance test suite, CI/CD checks.

### Risk 4: Platform-Specific Issues

**Risk**: Optimizations work on macOS but not Linux
**Mitigation**: Test on multiple platforms, document platform differences.

## Alternative Approaches

### Alternative 1: Rewrite in Compiled Language

**Pros**: Maximum performance potential
**Cons**: High effort, maintenance burden, Python ecosystem loss
**Decision**: Defer until after Python optimizations exhausted

### Alternative 2: Distributed Builds

**Pros**: Scales to many files
**Cons**: Complex setup, not needed for single talk builds
**Decision**: Out of scope for this CIP

### Alternative 3: Accept Current Performance

**Pros**: 4x is already good, focus effort elsewhere
**Cons**: Leaves performance on the table, users still wait 37s
**Decision**: Investigate first, then decide

## Related Work

- **CIP-0008**: Achieved 4x speedup via server mode
- **Requirement req0005**: Fast build operations
- **Other tools**: Hugo (milliseconds), Jekyll (seconds) - what can we learn?

## References

- CIP-0008: Integrate Lynguine Server Mode for Fast Builds
- `talks/_ai/benchmark_maketalk.sh`: Current benchmarking script
- `cip/cip0008.md`: Detailed performance analysis and methodology

## Dependencies

- Python profiling tools (`cProfile`, `line_profiler`)
- `pytest-benchmark` for performance testing
- GNU time or similar for detailed timing
- Existing CIP-0008 server mode infrastructure

## Timeline

- **Week 1**: Phase 1 (Profiling infrastructure) - 2-3 days
- **Week 2**: Phase 2 (Quick wins implementation) - 3-5 days
- **Week 3**: Phase 3 (Deep optimizations if needed) - variable
- **Week 4**: Phase 4 (Performance testing framework) - 2-3 days

**Total estimated effort**: 2-4 weeks depending on how deep we go.

## Progress

### Phase 1: Performance Profiling Infrastructure - ✅ COMPLETED (2026-01-04)

**Implementation**: 
- Created `scripts/profile-command` shell script for timing individual commands
- Implemented `profiler.py` with `BuildProfiler` class for hierarchical reporting
- Integrated profiler into `maketalk.py` and `makecv.py` with `--profile` flag
- Added `TIME_CMD` variable to Makefiles for command instrumentation

**Results**:
- Successfully profiled `ai-and-data-science.md` build
- Identified dependency scanning as #1 bottleneck (28s, 61.2% of total time)
- Identified `mdfield` calls as #2 bottleneck (7.8s, 17.0% of total time)
- Identified git pulls as #3 bottleneck (4.2s, 9.2% of total time)

**Backlog**: `2026-01-04_performance-profiling-infrastructure.md` - Completed

### Phase 2: Quick Win - Batch Dependency Extraction - ✅ COMPLETED (2026-01-04)

**Problem**: Dependency scanning was calling `dependencies` command 6+ times per build, each time reading and recursively processing all files from scratch (600-900 file operations per build).

**Implementation**:
- Added `dependencies batch` command to `lamd/dependencies.py`
- Extract all dependency types in one pass (inputs, diagrams, texdiagrams, docxdiagrams, dynamic)
- Output format: Prefixed lines (`DEPS:...`, `DIAGDEPS:...`, etc.)
- Updated `make-talk-flags.mk` and `make-cv-flags.mk` to use batch extraction
- Used temp file approach to avoid Make parsing issues with multiline output
- Added automatic cleanup of temp files

**Results**:
- **Dependency scanning**: 28.0s → 1.4-1.6s (**95% improvement, 18-20x faster**)
- **Overall build time**: 45-50s → 12-15s (**70% improvement, 3-4x faster**)
- Reduced from 19 dependency calls to 1 batch call (90% reduction)

**Testing**:
- Added comprehensive unit tests for batch command
- Verified with multiple talk files (ai-and-data-science.md, the-atomic-human-cses-aru-christmas.md)
- All existing tests still pass

**Documentation**:
- Updated docstrings in `dependencies.py` with batch command documentation
- Added tests in `tests/unit/test_dependencies.py`

**Backlog**: `2026-01-04_optimize-dependency-scanning.md` - Completed

## Phase 2b: Batch mdfield Calls (Completed)

**Date**: 2026-01-04

**Implementation**: Created `mdfield batch` command to extract all fields in one call
- Added `batch` subcommand to `lamd/mdfield.py`
- Modified `make-talk-flags.mk` (21 fields → 1 batch call)
- Modified `make-cv-flags.mk` (25 fields → 1 batch call)  
- Same pattern as dependencies batch: write to temp file, parse with grep/sed

**Performance Results**:
- `ai-and-data-science.md`: 3-4s → 0.12s (25-35x speedup)
- `the-atomic-human-cses-aru-christmas.md`: 6-7s → 0.17s (35-40x speedup)
- mdfield calls dropped from #1 bottleneck (39%) to minimal (2.6%)
- Overall build time: 12s → 6.75s (~45% improvement)

**Testing**: All 12 tests passing, including 6 new batch tests

**Backlog**: `2026-01-04_parallel-mdfield-calls.md` - Completed

### Remaining Bottlenecks (After Phase 2)

Current profile showed:
1. **Git pulls**: 2.9s (43% of total) - **→ Phase 3 implemented!**
2. **mdfield calls**: 1.8s (22% of total) - now acceptable
3. **Dependency scanning**: 1.3s (17% of total) - acceptable

## Phase 3: Smart Git Pull Caching (Completed)

**Date**: 2026-01-04

**Problem**: Git pulls were taking 2.8-4.2s per build even when repositories were already up-to-date. This was because we were contacting remote servers on every build.

**Solution**: Time-based caching
- Check `FETCH_HEAD` timestamp before fetching
- Only check remote if > 1 hour since last fetch
- Use `git rev-parse --git-dir` to find actual git directory (handles subdirectories)
- Apply to both dependency repos (snippetsdir, bibdir) and local repo

**Implementation**:
```python
# Find git root (works in subdirectories)
git_root = subprocess.run(["git", "rev-parse", "--git-dir"], ...)
fetch_head = os.path.join(git_dir, "FETCH_HEAD")

# Check timestamp
if os.path.exists(fetch_head):
    fetch_age = time.time() - os.path.getmtime(fetch_head)
    if fetch_age < 3600:  # 1 hour
        skip_remote_check = True
```

**Performance Results**:
- Dependency git pulls: 2.8s → 0.0s (eliminated!)
- Local git pull: 1.4s → 0.03s (98% reduction!)
- Overall build time: 7.9s → 3.4-3.7s (53% speedup)

**Testing**: Verified on multiple talk files:
- `the-atomic-human-cses-aru-christmas.md`: 7.9s → 3.7s
- `ai-and-data-science.md`: ~7s → 3.4s
- Second builds within an hour: git overhead drops to ~0.03s total

**Backlog**: `2026-01-04_conditional-git-pulls.md` - Completed

**Cumulative Impact - CIP-0009 Complete**:
- Phase 1 (Dependency batch): 28s → 1.4s (95% speedup)
- Phase 2 (mdfield batch): 3-4s → 1.2s (67% speedup)
- Phase 3 (Smart git pulls): 4.2s → 0.03s (99% speedup)
- **Overall: 45s → 3.4s (92% improvement, 13.2x faster!)**

**Target Achievement**: ✅ **Exceeded!**
- Original target: 6-8x speedup
- Achieved: **13.2x speedup**
- Build time reduced from 45s to 3.4s

## Next Steps

1. Get feedback on approach and priorities
2. Implement profiling infrastructure (`--profile` flag)
3. Run comprehensive benchmarks to establish baseline
4. Identify and implement top 3 quick wins
5. Measure results and decide on next phase

## Author

LAMD Development Team

## Date

2026-01-04

