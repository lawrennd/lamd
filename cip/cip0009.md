---
author: "lawrennd"
created: "2026-01-04"
id: "0009"
last_updated: "2026-01-04"
status: "Proposed"
related_requirements: ["0005"]
tags:
- cip
- performance
- optimization
- benchmarking
title: "Further Performance Optimization: Close the Gap"
---

# CIP-0009: Further Performance Optimization: Close the Gap

## Summary

While CIP-0008 achieved a 4x overall speedup (151s → 37s), there's a significant gap between the **per-call speedup (13.9x)** and the **overall pipeline speedup (4x)**. This CIP proposes systematic benchmarking and optimization to close that gap and achieve even faster builds.

## Status

**Current**: Proposed

## Motivation

### Current Performance (CIP-0008 Baseline)

| Metric | Before | After | Speedup |
|--------|--------|-------|---------|
| **Per mdfield call** | 1.25s | 0.09s | **13.9x** |
| **Overall talk build** | 151s | 37s | **4.0x** |

**The Gap**: 13.9x vs 4x suggests ~75% of build time is spent on non-mdfield operations.

### Why Optimize Further?

1. **User Experience**: Faster iteration cycles for talk/CV development
2. **Efficiency**: Reduce resource consumption for large-scale builds
3. **Understanding**: Identify other bottlenecks in the pipeline
4. **Best Practices**: Establish performance baseline for future work

### What's Taking Time?

For a 37-second build with 22 mdfield calls:
- **mdfield calls**: 22 × 0.09s ≈ **2s** (5% of total time)
- **Other operations**: 37s - 2s ≈ **35s** (95% of total time)

**Unknown**: What are those other 35 seconds doing?

## Objectives

### Primary Goals

1. **Profile the entire build pipeline** to identify bottlenecks
2. **Categorize time spent** in each build phase
3. **Optimize high-impact operations** (Pareto principle: 80/20)
4. **Achieve 6-8x overall speedup** (target: 151s → 20-25s range)

### Secondary Goals

5. **Establish performance testing framework** for regression detection
6. **Document best practices** for fast builds
7. **Create optimization guide** for users

## Proposed Approach

### Phase 1: Comprehensive Profiling

**Instrument the build pipeline** to measure:

1. **maketalk/makecv setup time**:
   - Argument parsing
   - Config file loading (`_lamd.yml`)
   - Interface validation
   - Git operations (`git pull` on dependencies)

2. **Makefile generation time**:
   - File I/O
   - Template processing

3. **Make execution time** (breakdown):
   - Each mdfield call (already optimized)
   - Preprocessor (`mdpp`) execution
   - Dependency scanning (`dependencies` command)
   - File I/O and cache checks
   - Pandoc execution (slides, notes, etc.)
   - Bibliography processing
   - Diagram copying/processing

4. **External tool overhead**:
   - Git operations
   - File system operations
   - Process spawning

**Tool**: Add `--profile` flag to `maketalk`/`makecv`:
```bash
maketalk my-talk.md --profile
# Outputs detailed timing breakdown
```

### Phase 2: Low-Hanging Fruit

**Quick wins** (likely candidates based on typical build patterns):

1. **Parallel mdfield calls**:
   - Currently: 22 sequential calls in Makefile
   - Proposal: Batch or parallelize using GNU make's `-j` flag
   - Expected: 2s → 0.2s (if 10x parallelism)

2. **Reduce git pulls**:
   - Currently: Pulls on every build
   - Proposal: Check if already up-to-date first
   - Expected: Save 2-5s per build

3. **Cache preprocessor output**:
   - Currently: Preprocessor runs on every build
   - Proposal: Cache based on file timestamps/hashes
   - Expected: Save 5-10s on unchanged files

4. **Optimize dependency scanning**:
   - Currently: Scans all includes/snippets
   - Proposal: Cache dependency graph
   - Expected: Save 2-5s

5. **Lazy config loading**:
   - Currently: Loads full Interface on every mdfield call
   - Already optimized via server mode
   - Check if any remaining instances

### Phase 3: Deep Optimizations

**Bigger changes** (if needed to hit target):

1. **Incremental builds**:
   - Only regenerate changed outputs
   - Smart dependency tracking
   - Expected: 10-20x for unchanged files

2. **Server-mode for other commands**:
   - `dependencies` command
   - `mdpp` preprocessor
   - `flags` command
   - Expected: 2-5x for these operations

3. **Compiled preprocessor**:
   - Port `mdpp` to compiled language (Rust?)
   - Expected: 5-10x for preprocessing

4. **Parallel pandoc execution**:
   - Generate multiple formats simultaneously
   - Expected: 2x for multi-format builds

### Phase 4: Performance Testing Framework

**Automated regression detection**:

```bash
# Performance test suite
pytest tests/performance/ -v

# Benchmark against baseline
pytest tests/performance/ --benchmark-only
```

**What to test**:
- Single talk build time
- Multi-talk batch build time
- CV build time
- Cold start vs warm cache
- Different talk sizes (small, medium, large)

## Success Criteria

### Minimum Success (Phase 1-2)

- [ ] Complete profiling of build pipeline
- [ ] Categorize time spent in each phase
- [ ] Implement at least 3 quick wins
- [ ] Achieve **5x overall speedup** (151s → 30s)

### Target Success (Phase 1-3)

- [ ] Achieve **6-8x overall speedup** (151s → 20-25s)
- [ ] Document all bottlenecks and optimizations
- [ ] Performance testing framework in place
- [ ] No regression in functionality or reliability

### Stretch Goals (Phase 4)

- [ ] Achieve **10x overall speedup** (151s → 15s)
- [ ] Incremental builds working
- [ ] Performance monitoring in CI/CD

## Implementation Plan

### Step 1: Add Profiling Infrastructure

Create `lamd/profiler.py`:
```python
import time
from contextlib import contextmanager

class BuildProfiler:
    def __init__(self):
        self.timings = {}
    
    @contextmanager
    def measure(self, operation: str):
        start = time.perf_counter()
        yield
        elapsed = time.perf_counter() - start
        self.timings[operation] = elapsed
    
    def report(self):
        total = sum(self.timings.values())
        for op, elapsed in sorted(self.timings.items(), key=lambda x: -x[1]):
            pct = (elapsed / total) * 100
            print(f"{op:40s}: {elapsed:6.2f}s ({pct:5.1f}%)")
```

Usage in `maketalk.py`:
```python
profiler = BuildProfiler()

with profiler.measure("Config Loading"):
    iface = Interface.from_file(...)

with profiler.measure("Git Pull Dependencies"):
    os.system("git pull")

# ... etc

if args.profile:
    profiler.report()
```

### Step 2: Benchmark Current State

Run comprehensive benchmarks:
```bash
# Small talk (few includes)
time maketalk small-talk.md --profile

# Medium talk (typical)
time maketalk medium-talk.md --profile

# Large talk (many includes, diagrams)
time maketalk large-talk.md --profile

# CV build
time makecv my-cv.md --profile
```

Document baseline in `docs/performance-baseline.md`.

### Step 3: Implement Quick Wins

Prioritize by ROI (impact / effort):
1. Conditional git pulls (high impact, low effort)
2. Parallel mdfield calls (medium impact, low effort)
3. Cache dependency scans (medium impact, medium effort)

### Step 4: Measure and Iterate

After each optimization:
- Re-run benchmarks
- Document speedup
- Check for regressions
- Commit with performance data

### Step 5: Performance Testing

Create `tests/performance/test_build_performance.py`:
```python
import pytest
from time import perf_counter

@pytest.mark.benchmark
def test_talk_build_performance(tmp_path):
    """Benchmark typical talk build time."""
    # Setup test talk file
    talk_file = create_test_talk(tmp_path, size="medium")
    
    # Measure build time
    start = perf_counter()
    result = subprocess.run(["maketalk", str(talk_file)])
    elapsed = perf_counter() - start
    
    # Assert performance threshold
    assert result.returncode == 0
    assert elapsed < 30.0, f"Build took {elapsed}s (target: <30s)"
```

## Potential Bottlenecks to Investigate

### 1. Git Operations

**Hypothesis**: `git pull` on every build adds 2-5s
**Test**: Compare build time with/without git operations
**Fix**: Only pull if not up-to-date

### 2. File I/O

**Hypothesis**: Reading/writing many small files is slow
**Test**: Profile file operations
**Fix**: Buffer writes, minimize reads

### 3. Process Spawning

**Hypothesis**: Spawning many subprocesses has overhead
**Test**: Count subprocess calls, measure overhead
**Fix**: Batch operations, use server mode for more tools

### 4. Preprocessor

**Hypothesis**: `mdpp` is slow for large files with many includes
**Test**: Profile preprocessor separately
**Fix**: Cache results, optimize hot paths, or rewrite in compiled language

### 5. Dependency Scanning

**Hypothesis**: Scanning all includes/snippets is redundant
**Test**: Profile `dependencies` command
**Fix**: Cache dependency graph, only rescan on file changes

### 6. Pandoc Execution

**Hypothesis**: Pandoc is slow but necessary
**Test**: Measure pandoc time separately
**Fix**: Parallel execution for multiple formats, check if newer version is faster

## Risks and Mitigations

### Risk 1: Premature Optimization

**Risk**: Optimizing wrong thing, no real benefit
**Mitigation**: Profile first, optimize second. Data-driven decisions.

### Risk 2: Complexity Increase

**Risk**: Caching and incremental builds add complexity
**Mitigation**: Start with simple optimizations. Add complexity only if needed for target.

### Risk 3: Regressions

**Risk**: Optimizations break functionality
**Mitigation**: Comprehensive testing, performance test suite, CI/CD checks.

### Risk 4: Platform-Specific Issues

**Risk**: Optimizations work on macOS but not Linux
**Mitigation**: Test on multiple platforms, document platform differences.

## Alternative Approaches

### Alternative 1: Rewrite in Compiled Language

**Pros**: Maximum performance potential
**Cons**: High effort, maintenance burden, Python ecosystem loss
**Decision**: Defer until after Python optimizations exhausted

### Alternative 2: Distributed Builds

**Pros**: Scales to many files
**Cons**: Complex setup, not needed for single talk builds
**Decision**: Out of scope for this CIP

### Alternative 3: Accept Current Performance

**Pros**: 4x is already good, focus effort elsewhere
**Cons**: Leaves performance on the table, users still wait 37s
**Decision**: Investigate first, then decide

## Related Work

- **CIP-0008**: Achieved 4x speedup via server mode
- **Requirement req0005**: Fast build operations
- **Other tools**: Hugo (milliseconds), Jekyll (seconds) - what can we learn?

## References

- CIP-0008: Integrate Lynguine Server Mode for Fast Builds
- `talks/_ai/benchmark_maketalk.sh`: Current benchmarking script
- `cip/cip0008.md`: Detailed performance analysis and methodology

## Dependencies

- Python profiling tools (`cProfile`, `line_profiler`)
- `pytest-benchmark` for performance testing
- GNU time or similar for detailed timing
- Existing CIP-0008 server mode infrastructure

## Timeline

- **Week 1**: Phase 1 (Profiling infrastructure) - 2-3 days
- **Week 2**: Phase 2 (Quick wins implementation) - 3-5 days
- **Week 3**: Phase 3 (Deep optimizations if needed) - variable
- **Week 4**: Phase 4 (Performance testing framework) - 2-3 days

**Total estimated effort**: 2-4 weeks depending on how deep we go.

## Next Steps

1. Get feedback on approach and priorities
2. Implement profiling infrastructure (`--profile` flag)
3. Run comprehensive benchmarks to establish baseline
4. Identify and implement top 3 quick wins
5. Measure results and decide on next phase

## Author

LAMD Development Team

## Date

2026-01-04

